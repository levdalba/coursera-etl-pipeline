# Coursera ETL Pipeline

This repository contains an ETL pipeline that extracts course data from the Coursera API, transforms it, and loads it into Google Cloud Storage (GCS) and BigQuery. The pipeline is deployed as a Cloud Run service and triggered daily via Cloud Scheduler.

## Overview
The ETL pipeline performs the following steps:
- **Extract**: Fetches course data from the Coursera API using a GraphQL query.
- **Transform**: Processes the API response into a structured format, extracting fields like `collection_label`, `collection_id`, `course_name`, `course_id`, `slug`, and `url`.
- **Load**:
  - Saves the raw API response as a JSON file (`coursera_response.json`) and the transformed data as a CSV file (`coursera_courses.csv`) to a GCS bucket (`etlpipelinehomeworkhbcloudrun`).
  - Loads the transformed data into a BigQuery table (`training-triggering-pipeline.coursera_data.courses`).

The pipeline is deployed on Cloud Run and scheduled to run daily at midnight UTC using Cloud Scheduler.

## Prerequisites
- Google Cloud SDK installed and authenticated (`gcloud auth login`).
- A Google Cloud project with billing enabled (e.g., `training-triggering-pipeline`).
- IAM permissions for Cloud Run, Cloud Build, Cloud Scheduler, BigQuery, and Cloud Storage.
- Git installed to clone the repository.

## Repository Architecture
The repository is structured as follows:
```
Coursera-ETL-Pipeline/
│── .dockerignore             # Files to ignore in Docker builds
│── .gitattributes            # Git attributes for repository configuration
│── .gitignore                # Files to ignore in Git commits
│── cloudbuild.yaml           # Cloud Build configuration for deployment
│── coursera_response.json    # Sample raw API response (generated by pipeline)
│── Dockerfile                # Docker configuration for Cloud Run deployment
│── main.py                   # Core ETL logic (extract, transform, load)
│── requirements.txt          # Python dependencies
```

## Setup Instructions

### 1. Clone the Repository
Clone this repository to your local machine:
```bash
git clone https://github.com/levandalbashvili/coursera-etl-pipeline.git
cd coursera-etl-pipeline
```

### 2. Configure Your Google Cloud Project
Set your Google Cloud project ID:
```bash
gcloud config set project training-triggering-pipeline
```

### 3. Update the API Token
The pipeline uses a static x-csrf3-token to authenticate with the Coursera API. This token expires frequently, so you need to update it before deployment:

1. Open https://www.coursera.org in a browser.
2. Open Developer Tools (F12) > Network tab.
3. Search for "machine learning" to trigger a DiscoveryCollections request.
4. Copy the x-csrf3-token from the request headers.
5. Update the HEADERS dictionary in `main.py` with the fresh token:

```python
HEADERS = {
    "accept": "application/json",
    "content-type": "application/json",
    "operation-name": "DiscoveryCollections",
    "origin": "https://www.coursera.org",
    "user-agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36",
    "x-csrf3-token": "your-fresh-token-here",  # Replace with the fresh token
}
```

### 4. Deploy to Cloud Run
Deploy the pipeline as a Cloud Run service using Cloud Build:
```bash
gcloud builds submit --config cloudbuild.yaml
```

- The service will be deployed in the `asia-southeast1` region.
- The service URL will be: `https://cloudrun-etl-974091072034.asia-southeast1.run.app`.
- The service is publicly accessible (`--allow-unauthenticated`). Secure it with IAM if needed.

### 5. Schedule with Cloud Scheduler
Set up a Cloud Scheduler job to trigger the Cloud Run service daily at midnight UTC:
```bash
gcloud scheduler jobs create http cloudrun-etl-job \
  --schedule="0 0 * * *" \
  --uri="https://cloudrun-etl-974091072034.asia-southeast1.run.app" \
  --http-method=GET \
  --location=us-central1 \
  --project=training-triggering-pipeline
```

Test the scheduler job by running it manually:
```bash
gcloud scheduler jobs run cloudrun-etl-job \
  --project=training-triggering-pipeline \
  --location=us-central1
```

**Note:** The Cloud Scheduler job triggers the service via HTTP and will not appear in the Cloud Run "Triggers" tab, which is for Eventarc triggers. To view the job, go to the Cloud Scheduler section in the Google Cloud Console.

## Configuration
**Environment Variables:** The pipeline uses the following defaults, which can be overridden via Cloud Run’s `--set-env-vars` flag during deployment:

- `GCS_BUCKET_NAME`: etlpipelinehomeworkhbcloudrun (GCS bucket for storing output files).
- `BQ_DATASET_ID`: coursera_data (BigQuery dataset).
- `BQ_TABLE_ID`: courses (BigQuery table).

Example of setting environment variables during deployment:
```bash
gcloud builds submit --config cloudbuild.yaml \
  --set-env-vars "GCS_BUCKET_NAME=your-bucket-name,BQ_DATASET_ID=your-dataset,BQ_TABLE_ID=your-table"
```

## Token Management
The current code uses a static `x-csrf3-token`, which expires frequently. For production, implement dynamic token retrieval using `beautifulsoup4` to scrape the token from Coursera’s website (see "Future Improvements" below).

## Monitoring
Verify the pipeline’s execution using the following commands:

Check Cloud Run Logs:
```bash
gcloud logging read "resource.type=cloud_run_revision resource.labels.service_name=cloudrun-etl resource.labels.location=asia-southeast1" \
  --project=training-triggering-pipeline \
  --limit=10 \
  --format="value(timestamp, textPayload)"
```

**Expected output:**
```text
2025-03-26T05:15:03.278681Z     Inserted 12 rows into BigQuery table courses
2025-03-26T05:15:02.461286Z     Table courses already exists
2025-03-26T05:15:02.131885Z     Dataset coursera_data already exists
2025-03-26T05:15:01.522117Z     Uploaded...
```

